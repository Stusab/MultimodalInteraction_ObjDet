{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.10/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image depicts a bustling urban scene. In the foreground, there are people interacting\n",
      "with their surroundings: one person appears to be sitting on a bench reading, while another is seated on the ground,\n",
      "seemingly absorbed in a phone. There’s also a figure lying down, which adds a sense of urgency or concern to the scene.\n",
      "Pigeons are scattered on the ground. \\n\\nIn the background, traffic is busy with cars and bicycles, and there are\n",
      "pedestrians crossing the street. The architecture reflects a mix of modern and older buildings, and the lighting\n",
      "suggests a late-day ambiance. Overall, it captures a dynamic moment in a city atmosphere.', role='assistant',\n",
      "function_call=None, tool_calls=None, refusal=None, annotations=[])\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'scene': 'A bustling urban street scene',\n",
       "  'elements': {'background': {'buildings': 'A mix of modern skyscrapers and older brick buildings',\n",
       "    'traffic_lights': 'A traffic light showing red',\n",
       "    'street': 'A busy street with vehicles in motion'},\n",
       "   'foreground': {'people': [{'position': 'sitting on a bench',\n",
       "      'activity': 'reading a newspaper',\n",
       "      'appearance': 'an older man in a suit'},\n",
       "     {'position': 'sitting on the ground',\n",
       "      'activity': 'using a smartphone',\n",
       "      'appearance': 'a young boy in a jacket'},\n",
       "     {'position': 'lying on the ground',\n",
       "      'activity': 'unconscious or resting',\n",
       "      'appearance': 'a young man in a red hoodie'},\n",
       "     {'position': 'walking',\n",
       "      'activity': 'holding a phone',\n",
       "      'appearance': 'a young woman in casual attire'},\n",
       "     {'position': 'walking',\n",
       "      'activity': 'playing guitar',\n",
       "      'appearance': 'a man in a hat'},\n",
       "     {'position': 'riding a bicycle',\n",
       "      'activity': 'biking through the street',\n",
       "      'appearance': 'a person in a black outfit'},\n",
       "     {'position': 'riding a scooter',\n",
       "      'activity': 'navigating the street',\n",
       "      'appearance': 'a woman in casual clothing'}],\n",
       "    'animals': [{'type': 'pigeons',\n",
       "      'position': 'on the ground near the bench'}],\n",
       "    'plants': {'type': 'flower pot',\n",
       "     'position': 'next to the boy on the ground',\n",
       "     'flowers': 'red geraniums'}}},\n",
       "  'atmosphere': {'lighting': 'Golden hour lighting creating a warm glow',\n",
       "   'mood': 'A mix of everyday life and a hint of drama'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['description'])\n",
      "dict_keys(['scene', 'elements', 'atmosphere'])\n"
     ]
    }
   ],
   "source": [
    "print(output.keys())\n",
    "print(output[\"description\"].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"background\": {\n",
      "    \"buildings\": \"A mix of modern skyscrapers and older brick buildings\",\n",
      "    \"traffic_lights\": \"A traffic light showing red\",\n",
      "    \"street\": \"A busy street with vehicles in motion\"\n",
      "  },\n",
      "  \"foreground\": {\n",
      "    \"people\": [\n",
      "      {\n",
      "        \"position\": \"sitting on a bench\",\n",
      "        \"activity\": \"reading a newspaper\",\n",
      "        \"appearance\": \"an older man in a suit\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"sitting on the ground\",\n",
      "        \"activity\": \"using a smartphone\",\n",
      "        \"appearance\": \"a young boy in a jacket\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"lying on the ground\",\n",
      "        \"activity\": \"unconscious or resting\",\n",
      "        \"appearance\": \"a young man in a red hoodie\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"walking\",\n",
      "        \"activity\": \"holding a phone\",\n",
      "        \"appearance\": \"a young woman in casual attire\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"walking\",\n",
      "        \"activity\": \"playing guitar\",\n",
      "        \"appearance\": \"a man in a hat\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"riding a bicycle\",\n",
      "        \"activity\": \"biking through the street\",\n",
      "        \"appearance\": \"a person in a black outfit\"\n",
      "      },\n",
      "      {\n",
      "        \"position\": \"riding a scooter\",\n",
      "        \"activity\": \"navigating the street\",\n",
      "        \"appearance\": \"a woman in casual clothing\"\n",
      "      }\n",
      "    ],\n",
      "    \"animals\": [\n",
      "      {\n",
      "        \"type\": \"pigeons\",\n",
      "        \"position\": \"on the ground near the bench\"\n",
      "      }\n",
      "    ],\n",
      "    \"plants\": {\n",
      "      \"type\": \"flower pot\",\n",
      "      \"position\": \"next to the boy on the ground\",\n",
      "      \"flowers\": \"red geraniums\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(output[\"description\"][\"elements\"], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_items = [\n",
    "    el for el in output[\"description\"][\"elements\"]\n",
    "    if \"position\" in el and any(word in el[\"position\"].lower() for word in [\"foreground\", \"front\", \"close\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'position': 'sitting on a bench',\n",
       "   'activity': 'reading a newspaper',\n",
       "   'appearance': 'an older man in a suit'},\n",
       "  {'position': 'sitting on the ground',\n",
       "   'activity': 'using a smartphone',\n",
       "   'appearance': 'a young boy in a jacket'},\n",
       "  {'position': 'lying on the ground',\n",
       "   'activity': 'unconscious or resting',\n",
       "   'appearance': 'a young man in a red hoodie'},\n",
       "  {'position': 'walking',\n",
       "   'activity': 'holding a phone',\n",
       "   'appearance': 'a young woman in casual attire'},\n",
       "  {'position': 'walking',\n",
       "   'activity': 'playing guitar',\n",
       "   'appearance': 'a man in a hat'},\n",
       "  {'position': 'riding a bicycle',\n",
       "   'activity': 'biking through the street',\n",
       "   'appearance': 'a person in a black outfit'},\n",
       "  {'position': 'riding a scooter',\n",
       "   'activity': 'navigating the street',\n",
       "   'appearance': 'a woman in casual clothing'}],\n",
       " 'animals': [{'type': 'pigeons', 'position': 'on the ground near the bench'}],\n",
       " 'plants': {'type': 'flower pot',\n",
       "  'position': 'next to the boy on the ground',\n",
       "  'flowers': 'red geraniums'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"elements\"][\"foreground\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this scene, there is a 15-year-old male who is lying down and unconscious, indicating a potential medical emergency. The Child Hospital should be alerted due to the age of the individual.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, there is no mention of a 16-year-old individual in the image analysis. The ages of the individuals listed are 30, 25, 15, 15, 20, 30, 40, and 35. Since there is no 16-year-old present, I cannot provide coordinates for that age group.\\n\\nIf you need assistance with another aspect of the scenario or have further questions, feel free to ask!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm unable to assist with that.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, doesn't work in a single, unified way.  Instead, it encompasses a broad range of techniques and approaches, all aiming to create systems that can perform tasks that typically require human intelligence.  Here's a breakdown of some key concepts:\n",
      "\n",
      "**1. Data as the Foundation:**  At the heart of most AI systems is data.  Massive amounts of data are used to train AI models. This data can be anything from images and text to sensor readings and financial transactions.  The quality and quantity of this data directly impact the performance of the AI.\n",
      "\n",
      "**2. Algorithms: The Recipes:** Algorithms are the sets of instructions that tell the AI how to process and learn from the data. These algorithms are mathematical formulas and procedures that allow the system to identify patterns, make predictions, and improve its performance over time.  Different algorithms are suited for different tasks.\n",
      "\n",
      "**3. Machine Learning (ML):  Learning from Data:**  Machine learning is a subset of AI where algorithms learn from data without explicit programming.  Instead of being explicitly told what to do, the algorithm identifies patterns and relationships in the data to build a model. This model can then be used to make predictions or decisions on new, unseen data.  There are several types of machine learning:\n",
      "\n",
      "* **Supervised Learning:** The algorithm is trained on a labeled dataset (data with known inputs and outputs).  The goal is to learn a mapping between inputs and outputs so it can predict outputs for new inputs.  Examples include image classification (identifying objects in images) and spam detection.\n",
      "* **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset (data without known outputs). The goal is to discover hidden patterns, structures, or relationships in the data. Examples include clustering (grouping similar data points) and dimensionality reduction (reducing the number of variables while preserving important information).\n",
      "* **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment. It receives rewards for desirable actions and penalties for undesirable actions. The goal is to learn a policy that maximizes cumulative rewards. Examples include game playing (e.g., AlphaGo) and robotics.\n",
      "\n",
      "**4. Deep Learning (DL):  Artificial Neural Networks:** Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data. These networks are inspired by the structure and function of the human brain.  Deep learning excels at tasks involving complex patterns and large datasets, such as image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "**5. Natural Language Processing (NLP): Understanding Language:** NLP focuses on enabling computers to understand, interpret, and generate human language. This includes tasks like machine translation, text summarization, sentiment analysis, and chatbot development.\n",
      "\n",
      "**6. Computer Vision: Seeing the World:** Computer vision allows computers to \"see\" and interpret images and videos. This involves tasks like object detection, image classification, and image segmentation.\n",
      "\n",
      "**In simple terms:** Imagine you want to teach a computer to identify cats in pictures.  You'd feed it thousands of pictures of cats (and maybe some pictures that aren't cats) – this is the data.  A machine learning algorithm would then analyze these pictures, identifying features like pointy ears, whiskers, and fur patterns.  It learns to associate these features with the label \"cat.\"  After training, when it sees a new picture, it can predict whether it contains a cat based on the features it learned.  Deep learning takes this a step further by using complex neural networks to extract even more intricate features from the images.\n",
      "\n",
      "\n",
      "This is a simplified explanation, and the field of AI is constantly evolving. However, it provides a basic understanding of the core principles and techniques behind how AI works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[698,328,964,620]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
